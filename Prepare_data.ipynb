{"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import sys\n","sys.path.append('/content/drive/MyDrive/cs229/sd_data/')\n","%cd drive/MyDrive/cs229/sd_data"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EXsjenlaxiI7","executionInfo":{"status":"ok","timestamp":1670622237557,"user_tz":480,"elapsed":1948,"user":{"displayName":"Kaien Yang","userId":"04753500752570754231"}},"outputId":"1bbc3aa4-4a8a-4824-cfb1-e8e63000a112"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","[Errno 2] No such file or directory: 'drive/MyDrive/cs229/sd_data'\n","/content/drive/.shortcut-targets-by-id/1hNmGL28ABDHXyODHwa494F88Q-xe1iut/sd_data\n"]}]},{"cell_type":"markdown","source":["### Prepare dataset"],"metadata":{"id":"3O0h26o8lDBu"}},{"cell_type":"code","source":["import pickle\n","with open(\"all_added_emb.pickle\", \"rb\") as f:\n","    all_added_emb = pickle.load(f)"],"metadata":{"id":"3J_GtxiWkhLy","executionInfo":{"status":"ok","timestamp":1670622246066,"user_tz":480,"elapsed":8513,"user":{"displayName":"Kaien Yang","userId":"04753500752570754231"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["%%capture\n","pip install git+https://github.com/openai/CLIP.git"],"metadata":{"id":"wOOsdaLZ11fQ","executionInfo":{"status":"ok","timestamp":1670622257978,"user_tz":480,"elapsed":11745,"user":{"displayName":"Kaien Yang","userId":"04753500752570754231"}}},"execution_count":20,"outputs":[]},{"cell_type":"code","source":["import clip\n","encoder, preprocess = clip.load(\"ViT-B/32\");"],"metadata":{"id":"TvgWpgiK5JY-","executionInfo":{"status":"ok","timestamp":1670622266299,"user_tz":480,"elapsed":8326,"user":{"displayName":"Kaien Yang","userId":"04753500752570754231"}}},"execution_count":21,"outputs":[]},{"cell_type":"code","source":["from PIL import Image\n","import numpy as np\n","import os\n","import torch\n","from torchvision import transforms\n","from tqdm import tqdm\n","\n","aug = transforms.Compose([\n","    transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4),\n","    transforms.GaussianBlur((5, 5)),\n","])\n","\n","CLIP_embeddings, TI_embeddings = [], []\n","\"\"\"\n","CLIP_embeddings is a (D, 512) numpy array containing the CLIP embeddings\n","of \"0.jpeg\" for various community-generated concepts.\n","\n","TI_embeddings is a (D, 768) numpy array containing the corresponding\n","textual inversion embedding for each concept in CLIP_embeddings.\n","\"\"\"\n","num_augs = 4\n","for concept in tqdm(os.listdir('./sd-concepts-library/')):\n","  concept_path = f\"./sd-concepts-library/{concept}/\"\n","  img_path = concept_path + \"3.jpeg\"\n","  if os.path.isdir(concept_path) and os.path.isfile(img_path) and concept in all_added_emb:\n","    img = Image.open(img_path)\n","    for _ in range(num_augs):\n","      img_tensor = preprocess(aug(img)).unsqueeze(0)\n","      CLIP_embeddings.append(encoder.encode_image(img_tensor).squeeze().detach().numpy())\n","      key = list(all_added_emb[concept].keys())[0]\n","      TI_embeddings.append(all_added_emb[concept][key].detach().numpy())\n","\n","CLIP_embeddings = np.stack(CLIP_embeddings)\n","TI_embeddings = np.stack(TI_embeddings)"],"metadata":{"id":"VO8K-ue6eiiN","executionInfo":{"status":"ok","timestamp":1670627518371,"user_tz":480,"elapsed":1587737,"user":{"displayName":"Kaien Yang","userId":"04753500752570754231"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"cc781e3f-910f-4aac-b525-9485678036d7"},"execution_count":35,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 723/723 [26:27<00:00,  2.20s/it]\n"]}]},{"cell_type":"code","source":["pickle.dump(CLIP_embeddings, open(\"3_AUG_CLIP_embeddings.pickle\", \"wb\"))\n","pickle.dump(TI_embeddings, open(\"3_AUG_TI_embeddings.pickle\", \"wb\"))\n","print(len(CLIP_embeddings))"],"metadata":{"id":"PcKHd0aDkO3T","executionInfo":{"status":"ok","timestamp":1670627665389,"user_tz":480,"elapsed":157,"user":{"displayName":"Kaien Yang","userId":"04753500752570754231"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"081d9111-ce0b-48e5-85cf-e0348b965216"},"execution_count":37,"outputs":[{"output_type":"stream","name":"stdout","text":["2476\n"]}]},{"cell_type":"code","source":["# from PIL import Image\n","# import os\n","\n","# CLIP_embeddings, TI_embeddings = [], []\n","# for concept in os.listdir('./sd-concepts-library/'):\n","#   concept_CLIP_embeddings = []\n","#   concept_path = f\"./sd-concepts-library/{concept}/\"\n","#   if os.path.isdir(concept_path) and concept in all_added_emb:\n","#     for f in os.listdir(concept_path):\n","#       if f[-5:] == \".jpeg\":\n","#         img = Image.open(concept_path+f)\n","#         concept_CLIP_embeddings.append(preprocess(img))\n","#   if len(concept_CLIP_embeddings) != 0:\n","#     concept_CLIP_embeddings = encoder.encode_image(torch.stack(concept_CLIP_embeddings))\n","#     CLIP_embeddings.append(concept_CLIP_embeddings)\n","#     key = list(all_added_emb[concept].keys())[0]\n","#     TI_embeddings.append(all_added_emb[concept][key].detach().numpy())"],"metadata":{"id":"HkR6DUHs5Oiu"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}